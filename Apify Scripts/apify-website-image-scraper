import asyncio
import os
from urllib.parse import urljoin
import httpx
from crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext
from apify import Actor

# Configuration
START_URL = "https://apify.com/store"
# For local execution
PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(PROJECT_DIR, "downloaded_images")
DATASET_FILE = os.path.join(PROJECT_DIR, "storage", "datasets", "default", "dataset.jsonl")

async def main():
    async with Actor:
        # For local execution: Create directories
        if os.path.exists(PROJECT_DIR):  # Only for local runs
            os.makedirs(OUTPUT_DIR, exist_ok=True)
            os.makedirs(os.path.dirname(DATASET_FILE), exist_ok=True)

        # Initialize crawler
        crawler = BeautifulSoupCrawler(
            max_requests_per_crawl=100,
            max_request_retries=0,
        )

        @crawler.router.default_handler
        async def request_handler(context: BeautifulSoupCrawlingContext) -> None:
            context.log.info(f"Processing {context.request.url}")
            soup = context.soup
            image_urls = []

            # Extract image URLs
            for img_tag in soup.find_all("img"):
                img_src = img_tag.get("src")
                if img_src:
                    absolute_url = urljoin(context.request.url, img_src)
                    image_urls.append(absolute_url)

            context.log.info(f"Found {len(image_urls)} images on {context.request.url}")

            # Process images
            for idx, img_url in enumerate(image_urls):
                try:
                    async with httpx.AsyncClient() as client:
                        response = await client.get(img_url, headers={"User-Agent": "Mozilla/5.0"})
                        response.raise_for_status()

                    file_extension = img_url.split(".")[-1].split("?")[0][:4] or "jpg"
                    file_name = f"image_{idx}_{context.request.id}.{file_extension}"
                    file_path = os.path.join(OUTPUT_DIR, file_name)

                    # For local execution: Save image
                    if os.path.exists(PROJECT_DIR):
                        with open(file_path, "wb") as f:
                            f.write(response.content)

                    # Push metadata to Apify dataset
                    dataset_entry = {
                        "image_url": img_url,
                        "file_name": f"images/{file_name}",
                        "source_page": context.request.url,
                        "status": "success"
                    }
                    context.log.info(f"Pushing data: {dataset_entry}")
                    await Actor.push_data(dataset_entry)

                    # For local execution: Write to JSONL
                    if os.path.exists(PROJECT_DIR):
                        with open(DATASET_FILE, "a") as f:
                            import json
                            f.write(json.dumps(dataset_entry) + "\n")

                    context.log.info(f"Processed: {file_name}")

                except httpx.HTTPError as e:
                    context.log.error(f"Failed to download {img_url}: {e}")
                    dataset_entry = {
                        "image_url": img_url,
                        "file_name": None,
                        "source_page": context.request.url,
                        "status": "failed_download",
                        "error": str(e)
                    }
                    context.log.info(f"Pushing data: {dataset_entry}")
                    await Actor.push_data(dataset_entry)
                    if os.path.exists(PROJECT_DIR):
                        with open(DATASET_FILE, "a") as f:
                            import json
                            f.write(json.dumps(dataset_entry) + "\n")

        # Run the crawler
        await crawler.run([START_URL])

if __name__ == "__main__":
    asyncio.run(main())
